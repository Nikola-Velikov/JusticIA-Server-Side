

# from pymongo import MongoClient
# from elasticsearch import Elasticsearch, helpers
# from elasticsearch.helpers import BulkIndexError
# import json
# import re
# import ast
# import google.generativeai as genai
# from bson import ObjectId
# from fastapi import FastAPI
# from pydantic import BaseModel
#
# # üß† Configure Gemini
# genai.configure(api_key="AIzaSyAsO2plQSBkJrv0EAYv7LMCbd3XZYnaeng")
# gemini_model = genai.GenerativeModel("gemini-2.5-flash")
#
# # ‚öôÔ∏è Database configuration
# MONGO_URI = "mongodb://mongo:kpBxiANKRFwHbakPiIIiVUgbzCFFsvyr@tramway.proxy.rlwy.net:30965"
# MONGO_DB = "legaldb"
# ES_URL = "https://elasticsearch-production-e1d2.up.railway.app"
# MONGO_COLLECTIONS = ["constitution", "codex", "laws", "implementableRegulations", "regulations", "rules"]
# MAX_RETRIES = 5
#
# mongo_client = MongoClient(MONGO_URI)
# mongo_db = mongo_client[MONGO_DB]
# es = Elasticsearch(
#     ES_URL,
#     verify_certs=False,
#     request_timeout=60,
#     retry_on_timeout=True,
#     max_retries=3
# )
#
# # ‚öñÔ∏è FastAPI setup
# app = FastAPI(title="JusticIA API", description="Legal AI Assistant API", version="1.0.0")
#
#
# # ‚úÖ CLEAN + INDEX FUNCTION
# def clean_document(doc):
#     """Recursively sanitize MongoDB document keys for Elasticsearch."""
#     clean_doc = {}
#     for k, v in doc.items():
#         clean_key = re.sub(r'[.$]', '_', k)
#         if isinstance(v, dict):
#             clean_doc[clean_key] = clean_document(v)
#         elif isinstance(v, list):
#             clean_doc[clean_key] = [clean_document(i) if isinstance(i, dict) else i for i in v]
#         else:
#             clean_doc[clean_key] = v
#     return clean_doc
#
#
# def index_mongo_to_es():
#     """Indexes all MongoDB collections into Elasticsearch safely and completely."""
#     for coll_name in MONGO_COLLECTIONS:
#         collection = mongo_db[coll_name]
#         docs = collection.find()
#         actions = []
#         error_count = 0
#         total_indexed = 0
#
#         print(f"üöÄ Indexing collection: {coll_name}")
#
#         for doc in docs:
#             try:
#                 doc_id = str(doc["_id"])
#                 doc.pop("_id", None)
#
#                 for key, value in doc.items():
#                     if isinstance(value, ObjectId):
#                         doc[key] = str(value)
#                     elif hasattr(value, "isoformat"):
#                         doc[key] = value.isoformat()
#
#                 doc = clean_document(doc)
#
#                 actions.append({
#                     "_index": coll_name.lower(),
#                     "_id": doc_id,
#                     "_source": doc
#                 })
#
#                 if len(actions) >= 500:
#                     helpers.bulk(es, actions, raise_on_error=False, request_timeout=120)
#                     total_indexed += len(actions)
#                     actions = []
#             except Exception as e:
#                 error_count += 1
#                 print(f"‚ö†Ô∏è Skipped one doc in {coll_name}: {e}")
#
#         if actions:
#             try:
#                 helpers.bulk(es, actions, raise_on_error=False, request_timeout=120)
#                 total_indexed += len(actions)
#             except BulkIndexError as e:
#                 print(f"‚ùå Bulk index error in {coll_name}")
#                 for err in e.errors[:5]:
#                     print(json.dumps(err, indent=2, ensure_ascii=False))
#             except Exception as e:
#                 print(f"üí• Unexpected error in {coll_name}: {e}")
#
#         print(f"‚úÖ Indexed {total_indexed} documents from {coll_name}")
#         if error_count:
#             print(f"‚ö†Ô∏è Skipped {error_count} invalid docs in {coll_name}")
#
#     print("üéØ All MongoDB collections successfully indexed into Elasticsearch!")
#
#
# def ask_gemini(prompt):
#     try:
#         response = gemini_model.generate_content(prompt)
#         return response.text
#     except Exception as e:
#         print("–ù–µ –º–æ–∂–µ –¥–∞ —Å–µ –æ—Ç–≥–æ–≤–æ—Ä–∏ –Ω–∞ –≤—ä–ø—Ä–æ—Å–∞ –≤–∏:", e)
#         return None
#
#
# def extract_term_and_collection(question):
#     prompt = f"""
# –¢–∏ —Å–∏ –∏–Ω—Ç–µ–ª–∏–≥–µ–Ω—Ç–µ–Ω –±—ä–ª–≥–∞—Ä—Å–∫–∏ –ø—Ä–∞–≤–µ–Ω –∞—Å–∏—Å—Ç–µ–Ω—Ç. –¢–≤–æ—è—Ç–∞ –∑–∞–¥–∞—á–∞ –µ –¥–∞ –∏–∑–≤–ª–µ—á–µ—à:
# 1Ô∏è‚É£ –æ—Å–Ω–æ–≤–Ω–∏—è –ø—Ä–∞–≤–µ–Ω —Ç–µ—Ä–º–∏–Ω (–∫–ª—é—á–æ–≤–∞—Ç–∞ –¥—É–º–∞, –ø–æ –∫–æ—è—Ç–æ —Ç—Ä—è–±–≤–∞ –¥–∞ —Å–µ —Ç—ä—Ä—Å–∏ –≤ –∑–∞–∫–æ–Ω–∏—Ç–µ);
# 2Ô∏è‚É£ –µ–¥–Ω–∞ –∏–ª–∏ –ø–æ–≤–µ—á–µ –∫–æ–ª–µ–∫—Ü–∏–∏ –æ—Ç –±–∞–∑–∞—Ç–∞ –¥–∞–Ω–Ω–∏, –∫–æ–∏—Ç–æ –µ –Ω–∞–π-–≤–µ—Ä–æ—è—Ç–Ω–æ –¥–∞ —Å—ä–¥—ä—Ä–∂–∞—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è—Ç–∞ –∑–∞ —Ç–æ–∑–∏ —Ç–µ—Ä–º–∏–Ω.
#
# üìö –ù–∞–ª–∏—á–Ω–∏ –∫–æ–ª–µ–∫—Ü–∏–∏ –∏ —Ç—è—Ö–Ω–æ—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ:
# - "constitution" ‚Üí –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è –Ω–∞ –†–µ–ø—É–±–ª–∏–∫–∞ –ë—ä–ª–≥–∞—Ä–∏—è (–æ—Å–Ω–æ–≤–Ω–∏ –ø—Ä–∞–≤–∞, –¥—ä—Ä–∂–∞–≤–Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ)
# - "codex" ‚Üí –ö–æ–¥–µ–∫—Å–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä –ù–∞–∫–∞–∑–∞—Ç–µ–ª–µ–Ω –∫–æ–¥–µ–∫—Å, –°–µ–º–µ–π–µ–Ω –∫–æ–¥–µ–∫—Å, –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏ –ø—Ä–æ—Ü–µ—Å—É–∞–ª–µ–Ω –∫–æ–¥–µ–∫—Å)
# - "laws" ‚Üí –ó–∞–∫–æ–Ω–∏ (–æ–±—â–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–∏ –∞–∫—Ç–æ–≤–µ)
# - "implementableRegulations" ‚Üí –ü—Ä–∞–≤–∏–ª–Ω–∏—Ü–∏ –∑–∞ –ø—Ä–∏–ª–∞–≥–∞–Ω–µ –Ω–∞ –∑–∞–∫–æ–Ω–∏
# - "regulations" ‚Üí –ü—Ä–∞–≤–∏–ª–Ω–∏—Ü–∏ (–≤—ä—Ç—Ä–µ—à–Ω–∏ –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–∞–Ω–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–∏ –∞–∫—Ç–æ–≤–µ)
# - "rules" ‚Üí –ù–∞—Ä–µ–¥–±–∏ –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
#
# üìñ –ó–∞–¥–∞—á–∞:
# –û—Ç —Å–ª–µ–¥–Ω–∏—è –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—Å–∫–∏ –≤—ä–ø—Ä–æ—Å –∏–∑–≤–ª–µ—á–∏:
# - –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏—è –ø—Ä–∞–≤–µ–Ω —Ç–µ—Ä–º–∏–Ω (–ø—Ä–∏–º–µ—Ä: "—Ä–∞–∑–≤–æ–¥", "–æ–±–∂–∞–ª–≤–∞–Ω–µ", "—Ç—Ä—É–¥–æ–≤ –¥–æ–≥–æ–≤–æ—Ä", "–≥–ª–æ–±–∞", "–¥–∞–Ω—ä—Ü–∏", "–ø—Ä–∞–≤–æ –Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–æ—Å—Ç")
# - —Å–ø–∏—Å—ä–∫ –æ—Ç –∫–æ–ª–µ–∫—Ü–∏–∏, –∫—ä–¥–µ—Ç–æ –µ –Ω–∞–π-–≤–µ—Ä–æ—è—Ç–Ω–æ –¥–∞ —Å–µ –Ω–∞–º–µ—Ä–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è—Ç–∞.
#
# ‚ùó –ê–∫–æ –≤—ä–ø—Ä–æ—Å—ä—Ç –Ω—è–º–∞ –ø—Ä–∞–≤–µ–Ω —Ö–∞—Ä–∞–∫—Ç–µ—Ä (–Ω–∞–ø—Ä. –ª–∏—á–µ–Ω, –∂–∏—Ç–µ–π—Å–∫–∏, –Ω–∞—É—á–µ–Ω, –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏ —Ç.–Ω.), –≤—ä—Ä–Ω–∏:
# {{
#   "term": "",
#   "collection": []
# }}
#
# üéØ –ü—Ä–∏–º–µ—Ä:
# –í—ä–ø—Ä–æ—Å: "–ö–∞–∫ —Å–µ –ø–æ–¥–∞–≤–∞ –∂–∞–ª–±–∞ —Å—Ä–µ—â—É —Å—ä–¥–µ–±–Ω–æ —Ä–µ—à–µ–Ω–∏–µ?"
# –†–µ–∑—É–ª—Ç–∞—Ç:
# {{
#   "term": "–æ–±–∂–∞–ª–≤–∞–Ω–µ",
#   "collection": ["codex", "laws"]
# }}
#
# ‚öñÔ∏è –í—ä–ø—Ä–æ—Å: "{question}"
#
# –û—Ç–≥–æ–≤–æ—Ä–∏ –°–ê–ú–û –≤—ä–≤ —Ñ–æ—Ä–º–∞—Ç JSON, –±–µ–∑ –æ–±—è—Å–Ω–µ–Ω–∏—è, –∫–æ–º–µ–Ω—Ç–∞—Ä–∏ –∏–ª–∏ —Ç–µ–∫—Å—Ç –ø—Ä–µ–¥–∏/—Å–ª–µ–¥ JSON.
# """
#     output = ask_gemini(prompt)
#
#     try:
#         json_start = output.find("{")
#         json_end = output.rfind("}") + 1
#         json_str = output[json_start:json_end]
#         parsed = json.loads(json_str)
#         return parsed.get("term", "").lower(), parsed.get("collection", [])
#     except Exception as e:
#         print("Failed to parse Gemini term response:", e)
#         return None, []
#
#
# def find_matching_indices(term, indices):
#     matched = []
#     for idx in indices:
#         if not idx:
#             continue
#         try:
#             res = es.search(index=idx, body={
#                 "query": {
#                     "multi_match": {
#                         "query": term,
#                         "fields": ["description"]
#                     }
#                 },
#                 "size": 1
#             })
#             if res.get("hits", {}).get("total", {}).get("value", 0) > 0:
#                 matched.append(idx)
#         except Exception as e:
#             print(f"‚ö†Ô∏è Error searching in index '{idx}': {e}")
#     return matched
#
#
# def generate_detailed_dsl(question, term, indices, excluded_terms=[]):
#     if not isinstance(indices[0], str):
#         indices = indices[0]
#     excluded = f" –ü—Ä–µ–¥–∏—à–Ω–∏ —Ç–µ—Ä–º–∏–Ω–∏ –±–µ–∑ —Ä–µ–∑—É–ª—Ç–∞—Ç: {', '.join(excluded_terms)}." if excluded_terms else ""
#     prompt = f"""
# –ò–∑—Ö–æ–¥–µ–Ω –≤—ä–ø—Ä–æ—Å: \"{question}\"
# –¢–µ–∫—É—â —Ç–µ—Ä–º–∏–Ω: \"{term}\".{excluded}
# –ì–µ–Ω–µ—Ä–∏—Ä–∞–π –¥–µ—Ç–∞–π–ª–Ω–∞ Elasticsearch DSL –∑–∞—è–≤–∫–∞ —Å 'highlight', —Ç—ä—Ä—Å–µ—â–∞ –≤ –ø–æ–ª–µ 'description'. –í—ä—Ä–Ω–∏ —Å–∞–º–æ JSON. –ù–ï –≤–∫–ª—é—á–≤–∞–π 'indices' –≤ JSON –∑–∞—è–≤–∫–∞—Ç–∞.
# """
#     output = ask_gemini(prompt)
#
#     try:
#         json_start = output.find("{")
#         json_end = output.rfind("}") + 1
#         json_text = output[json_start:json_end]
#         return json.loads(json_text)
#     except Exception as e:
#         print("DSL parse error in detailed_dsl:", e)
#         return {
#             "query": {
#                 "match": {
#                     "description": term
#                 }
#             },
#             "highlight": {
#                 "fields": {
#                     "description": {}
#                 }
#             }
#         }
#
#
# def extract_article_context(description, term):
#     pattern = r"(–ß–ª\..*?)(?=–ß–ª\.|$)"
#     matches = re.findall(pattern, description, flags=re.DOTALL)
#     return [m.strip() for m in matches if term.lower() in m.lower()]
#
#
# def summarize_results(question, chunks):
#     full_text = "\n\n".join(chunks)
#     prompt = f"""
# –ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è—Ç –ø–∏—Ç–∞: \"{question}\"
# –ù–∞–º–µ—Ä–µ–Ω–∏ —Å–∞ —Å–ª–µ–¥–Ω–∏—Ç–µ —á–ª–µ–Ω–æ–≤–µ:
# {full_text}
#
# –û–±–æ–±—â–∏ –≥–∏ –Ω–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∏, –∫–∞—Ç–æ –≥–æ–≤–æ—Ä–∏—à –≤ —Ç—Ä–µ—Ç–æ –ª–∏—Üe. –§–æ—Ä–º–∞—Ç–∞ —Ç—Ä—è–±–≤–∞ –¥–∞ –µ markdown –∏ –Ω–µ —Å–µ –æ–±—Ä—ä—â–∞–π –∫—ä–º –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è. –ê–∫–æ –≤—ä–ø—Ä–æ—Å—ä—Ç –∫–æ–π—Ç–æ –µ –ø–æ–ø–∏—Ç–∞–ª –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è –Ω—è–º–∞ –Ω–∏—â–æ –æ–±—â–æ —Å –ø—Ä–∞–≤–æ—Ç–æ –º—É –∫–∞–∂–∏, —á–µ –Ω–µ –º–æ–∂–µ—à –¥–∞ –æ—Ç–≥–æ–≤–æ—Ä–∏—à –Ω–∞ —Ç–æ–∑–∏ –≤—ä–ø—Ä–æ—Å.
# """
#     output = ask_gemini(prompt)
#     return output.strip()
#
#
# def generate_term_with_retries(question):
#     for attempt in range(MAX_RETRIES):
#         term, collection = extract_term_and_collection(question)
#         if not isinstance(collection, list):
#             collection = [collection]
#         if not term or not collection:
#             continue
#         matched_indices = find_matching_indices(term, collection)
#         if matched_indices:
#             return term, matched_indices, []
#     return None, [], []
#
#
# def handle_question(question):
#     term, matched_indices, failed_terms = generate_term_with_retries(question)
#
#     if not term:
#         return {"error": "–ù–µ –º–æ–∂–µ –¥–∞ —Å–µ –Ω–∞–º–µ—Ä–∏ —Ç–µ—Ä–º–∏–Ω —Å —Ä–µ–∑—É–ª—Ç–∞—Ç–∏."}
#     if not matched_indices:
#         return {"error": "–ù—è–º–∞ –∏–Ω–¥–µ–∫—Å–∏ —Å —Ä–µ–∑—É–ª—Ç–∞—Ç–∏ –∑–∞ —Ç–æ–∑–∏ —Ç–µ—Ä–º–∏–Ω."}
#
#     if not isinstance(matched_indices[0], str):
#         matched_indices = matched_indices[0]
#     matched_indices = [i for i in matched_indices if i]
#     indices_str = ",".join(matched_indices)
#
#     print(f"üîç Searching for term '{term}' in indices: {indices_str}")
#
#     detailed_dsl = generate_detailed_dsl(question, term, matched_indices)
#
#     try:
#         res = es.search(
#             index=indices_str,
#             body=detailed_dsl,
#
#
#         )
#
#     except Exception as e:
#         print(f"üí• Elasticsearch search error: {e}")
#         return {"error": f"–ù–µ—É—Å–ø–µ—à–Ω–æ —Ç—ä—Ä—Å–µ–Ω–µ –≤ Elasticsearch: {str(e)}"}
#
#     hits = res.get("hits", {}).get("hits", [])
#     if not hits:
#         return {"message": f"–ù—è–º–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏ –∑–∞ '{term}'."}
#
#     all_hits = []
#     sources = []
#
#     for hit in hits:
#         desc = hit["_source"].get("description", "")
#         chlen_matches = extract_article_context(desc, term)
#         all_hits.extend(chlen_matches)
#         sources.append({
#             "index": hit["_index"],
#             "title": hit["_source"].get("title", "–ë–µ–∑ –∑–∞–≥–ª–∞–≤–∏–µ")
#         })
#
#     summary = summarize_results(question, all_hits) if all_hits else "–ù—è–º–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏ —á–ª–µ–Ω–æ–≤–µ."
#
#     return {
#         "term": term,
#         "indices": matched_indices,
#         "results_count": len(all_hits),
#         "summary": summary,
#         "sources": sources,
#         "matches": all_hits
#     }
#
#
# # üß© FastAPI Routes
# class Question(BaseModel):
#     question: str
#
#
# @app.get("/")
# def home():
#     return {"message": "JusticIA API is running. POST your question to /generate"}
#
#
# @app.post("/index")
# def index_all_data():
#     index_mongo_to_es()
#     return {"message": "Data indexed successfully."}
#
#
# @app.post("/generate")
# def generate(payload: Question):
#     return handle_question(payload.question)
#

# look this should be array but its like a string and in there array fix it to be array everywhere and dont change anything else error: ‚ö†Ô∏è Error searching in index '['codex', 'laws']': BadRequestError(400, 'media_type_header_exception', 'Invalid media-type value on headers [Accept, Content-Type]', Accept version must be either version 8 or 7, but found 9. Accept=application/vnd.elasticsearch+json; compatible-with=9)
# INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK
#
#
from pymongo import MongoClient
from elasticsearch import Elasticsearch, helpers
from elasticsearch.helpers import BulkIndexError
import json
import re
import google.generativeai as genai
from bson import ObjectId
from fastapi import FastAPI
from pydantic import BaseModel

# üß† Configure Gemini
genai.configure(api_key="AIzaSyDFIm6r7BW-SFdngtGUd_76zUV2cVKXOl4")
gemini_model = genai.GenerativeModel("gemini-2.5-flash")

# ‚öôÔ∏è Database configuration
MONGO_URI = "mongodb://mongo:kpBxiANKRFwHbakPiIIiVUgbzCFFsvyr@tramway.proxy.rlwy.net:30965"
MONGO_DB = "legaldb"
ES_URL = "https://elasticsearch-production-e1d2.up.railway.app"
MONGO_COLLECTIONS = ["constitution", "codex", "laws", "implementableRegulations", "regulations", "rules"]
MAX_RETRIES = 5

mongo_client = MongoClient(MONGO_URI)
mongo_db = mongo_client[MONGO_DB]
es = Elasticsearch(
    ES_URL,
    verify_certs=False,
    request_timeout=60,
    retry_on_timeout=True,
    max_retries=3
)

# ‚öñÔ∏è FastAPI setup
app = FastAPI(title="JusticIA API", description="Legal AI Assistant API", version="1.0.0")


# ‚úÖ CLEAN + INDEX FUNCTION
def clean_document(doc):
    """Recursively sanitize MongoDB document keys for Elasticsearch."""
    clean_doc = {}
    for k, v in doc.items():
        clean_key = re.sub(r'[.$]', '_', k)
        if isinstance(v, dict):
            clean_doc[clean_key] = clean_document(v)
        elif isinstance(v, list):
            clean_doc[clean_key] = [clean_document(i) if isinstance(i, dict) else i for i in v]
        else:
            clean_doc[clean_key] = v
    return clean_doc


def index_mongo_to_es():
    """Indexes all MongoDB collections into Elasticsearch safely and completely."""
    for coll_name in MONGO_COLLECTIONS:
        collection = mongo_db[coll_name]
        docs = collection.find()
        actions = []
        error_count = 0
        total_indexed = 0

        print(f"üöÄ Indexing collection: {coll_name}")

        for doc in docs:
            try:
                doc_id = str(doc["_id"])
                doc.pop("_id", None)

                # Convert Mongo types
                for key, value in doc.items():
                    if isinstance(value, ObjectId):
                        doc[key] = str(value)
                    elif hasattr(value, "isoformat"):
                        doc[key] = value.isoformat()

                # Clean field names
                doc = clean_document(doc)

                actions.append({
                    "_index": coll_name.lower(),
                    "_id": doc_id,
                    "_source": doc
                })

                if len(actions) >= 500:
                    helpers.bulk(es, actions, raise_on_error=False, request_timeout=120)
                    total_indexed += len(actions)
                    actions = []
            except Exception as e:
                error_count += 1
                print(f"‚ö†Ô∏è Skipped one doc in {coll_name}: {e}")

        if actions:
            try:
                helpers.bulk(es, actions, raise_on_error=False, request_timeout=120)
                total_indexed += len(actions)
            except BulkIndexError as e:
                print(f"‚ùå Bulk index error in {coll_name}")
                for err in e.errors[:5]:
                    print(json.dumps(err, indent=2, ensure_ascii=False))
            except Exception as e:
                print(f"üí• Unexpected error in {coll_name}: {e}")

        print(f"‚úÖ Indexed {total_indexed} documents from {coll_name}")
        if error_count:
            print(f"‚ö†Ô∏è Skipped {error_count} invalid docs in {coll_name}")

    print("üéØ All MongoDB collections successfully indexed into Elasticsearch!")


def ask_gemini(prompt):
    try:
        response = gemini_model.generate_content(prompt)
        print("Term: ", response.text)
        return response.text
    except Exception as e:
        print("–ù–µ –º–æ–∂–µ –¥–∞ —Å–µ –æ—Ç–≥–æ–≤–æ—Ä–∏ –Ω–∞ –≤—ä–ø—Ä–æ—Å–∞ –≤–∏:", e)
        return None


def extract_term_and_collection(question):
    prompt = f"""
–¢–∏ —Å–∏ –∏–Ω—Ç–µ–ª–∏–≥–µ–Ω—Ç–µ–Ω –±—ä–ª–≥–∞—Ä—Å–∫–∏ –ø—Ä–∞–≤–µ–Ω –∞—Å–∏—Å—Ç–µ–Ω—Ç. –¢–≤–æ—è—Ç–∞ –∑–∞–¥–∞—á–∞ –µ –¥–∞ –æ–ø—Ä–µ–¥–µ–ª–∏—à:
–æ—Å–Ω–æ–≤–Ω–∏—è –ø—Ä–∞–≤–µ–Ω —Ç–µ—Ä–º–∏–Ω –æ—Ç –≤—ä–ø—Ä–æ—Å–∞ –∏ –¥–∞ –ì–µ–Ω–µ—Ä–∏—Ä–∞—à –Ω–∞–π –Ω–∞ –≤–∞–∂–Ω–æ—Ç–æ –¥–æ 2-3 –¥—É–º–∏ –∞–º–∞ –Ω–∞–∏—Å—Ç–∏–Ω–∞ –Ω–∞–π –Ω–∞–π –≤–∞–∂–Ω–æ—Ç–æ –ì–ï–ù–ï–†–ò–†–ê–® (—Å–∞–º–æ —Å—ä—â–µ—Å—Ç–≤–∏—Ç–µ–ª–Ω–æ –∏–ª–∏ –ø—Ä–∞–≤–Ω–æ –ø–æ–Ω—è—Ç–∏–µ, –Ω–µ –≥–ª–∞–≥–æ–ª),
 –∫–æ–ª–µ–∫—Ü–∏–∏—Ç–µ –æ—Ç –±–∞–∑–∞—Ç–∞, –∫—ä–¥–µ—Ç–æ –Ω–∞–π-–≤–µ—Ä–æ—è—Ç–Ω–æ —Å–µ —Å—ä–¥—ä—Ä–∂–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∑–∞ —Ç–æ–∑–∏ —Ç–µ—Ä–º–∏–Ω.

–ù–∞—Å–æ–∫–∏:
- –ò–≥–Ω–æ—Ä–∏—Ä–∞–π –≥–ª–∞–≥–æ–ª–∏ –∫–∞—Ç–æ "–æ–±–∂–∞–ª–≤–∞–Ω–µ", "–ø–æ–¥–∞–≤–∞–Ω–µ", "–∏–º–∞–º –ø—Ä–∞–≤–æ", "–ø–æ–ª—É—á–∞–≤–∞–º", "–ø—Ä–∞–≤—è".
- –ê–∫–æ –≤—ä–ø—Ä–æ—Å—ä—Ç —Å—ä–¥—ä—Ä–∂–∞ –∏ –¥–µ–π—Å—Ç–≤–∏–µ, –∏ –æ–±–µ–∫—Ç (–Ω–∞–ø—Ä. ‚Äû–ö–∞–∫–≤–∏  —Å–∞ –ø—Ä–∞–≤–æ–º–æ—â–∏—è –Ω–∞ –æ–±—â–æ—Ç–æ —Å—ä–±—Ä–∞–Ω–∏–µ –Ω–∞ –µ—Ç–∞–∂–Ω–∞—Ç–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–æ—Å—Ç?‚Äú), —Ç–µ—Ä–º–∏–Ω—ä—Ç –µ —Å–∞–º–æ –æ–±–µ–∫—Ç—ä—Ç ‚Üí ‚Äû–Ω–µ—Ç–∞–∂–Ω–∞—Ç–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–æ—Å—Ç‚Äú.
- –ê–∫–æ –Ω—è–º–∞ —è—Å–µ–Ω –æ–±–µ–∫—Ç, –∏–∑–±–µ—Ä–∏ –Ω–∞–π-–±–ª–∏–∑–∫–æ—Ç–æ –ø—Ä–∞–≤–Ω–æ –ø–æ–Ω—è—Ç–∏–µ.
- –ê–∫–æ –≤—ä–ø—Ä–æ—Å—ä—Ç –Ω–µ –µ –ø—Ä–∞–≤–µ–Ω ‚Äî –≤—ä—Ä–Ω–∏ –ø—Ä–∞–∑–µ–Ω JSON.

–ö–æ–ª–µ–∫—Ü–∏–∏:
- "constitution" ‚Üí –ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏—è
- "codex" ‚Üí –ö–æ–¥–µ–∫—Å–∏
- "laws" ‚Üí –ó–∞–∫–æ–Ω–∏
- "implementableRegulations" ‚Üí –ü—Ä–∞–≤–∏–ª–Ω–∏—Ü–∏ –∑–∞ –ø—Ä–∏–ª–∞–≥–∞–Ω–µ
- "regulations" ‚Üí –ü—Ä–∞–≤–∏–ª–Ω–∏—Ü–∏
- "rules" ‚Üí –ù–∞—Ä–µ–¥–±–∏

–§–æ—Ä–º–∞—Ç:
{{
  "term": "<–∫–ª—é—á–æ–≤ –ø—Ä–∞–≤–µ–Ω —Ç–µ—Ä–º–∏–Ω>",
  "collection": ["<–µ–¥–Ω–∞ –∏–ª–∏ –ø–æ–≤–µ—á–µ –æ—Ç: constitution, codex, laws, implementableRegulations, regulations, rules>"]
}}

–í—ä–ø—Ä–æ—Å: "{question}"
"""
    output = ask_gemini(prompt)
    try:
        json_start = output.find("{")
        json_end = output.rfind("}") + 1
        parsed = json.loads(output[json_start:json_end])
        term = parsed.get("term", "").lower()
        collection = parsed.get("collection", [])
        if isinstance(collection, str):
            collection = [collection]
        return term, collection
    except Exception as e:
        print("Failed to parse Gemini term response:", e)
        return None, []


def find_matching_indices(term, indices):
    """Find indices in Elasticsearch that contain the search term."""
    matched = []
    try:
        all_es_indices = es.indices.get_alias(name="*").keys()
    except Exception as e:
        print(f"‚ö†Ô∏è Could not retrieve ES indices: {e}")
        all_es_indices = []

    for idx in indices:
        if not idx:
            continue
        if idx not in all_es_indices:
            print(f"‚ö†Ô∏è Skipping missing index '{idx}' ‚Äî not found in Elasticsearch.")
            continue
        try:
            res = es.search(index=idx, body={
                "query": {
                    "multi_match": {
                        "query": term,
                        "fields": ["description"]
                    }
                },
                "size": 1
            })
            if res.get("hits", {}).get("total", {}).get("value", 0) > 0:
                matched.append(idx)
        except Exception as e:
            print(f"‚ö†Ô∏è Error searching in index '{idx}': {e}")
    return matched


def generate_detailed_dsl(question, term, indices, excluded_terms=[]):
    if not isinstance(indices[0], str):
        indices = indices[0]
    excluded = f" –ü—Ä–µ–¥–∏—à–Ω–∏ —Ç–µ—Ä–º–∏–Ω–∏ –±–µ–∑ —Ä–µ–∑—É–ª—Ç–∞—Ç: {', '.join(excluded_terms)}." if excluded_terms else ""
    prompt = f"""
–ò–∑—Ö–æ–¥–µ–Ω –≤—ä–ø—Ä–æ—Å: \"{question}\"
–¢–µ–∫—É—â —Ç–µ—Ä–º–∏–Ω: \"{term}\".{excluded}
–ì–µ–Ω–µ—Ä–∏—Ä–∞–π Elasticsearch DSL –∑–∞—è–≤–∫–∞ —Å highlight –∑–∞ –ø–æ–ª–µ 'description'. –í—ä—Ä–Ω–∏ —Å–∞–º–æ JSON.
"""
    output = ask_gemini(prompt)
    try:
        json_start = output.find("{")
        json_end = output.rfind("}") + 1
        return json.loads(output[json_start:json_end])
    except Exception:
        return {
            "query": {"match": {"description": term}},
            "highlight": {"fields": {"description": {}}}
        }


def extract_article_context(description, term):
    pattern = r"(–ß–ª\..*?)(?=–ß–ª\.|$)"
    matches = re.findall(pattern, description, flags=re.DOTALL)
    return [m.strip() for m in matches if term.lower() in m.lower()]


def summarize_results(question, chunks):
    full_text = "\n\n".join(chunks)
    prompt = f"""
–ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—è—Ç –ø–∏—Ç–∞: \"{question}\"
–ù–∞–º–µ—Ä–µ–Ω–∏ —Å–∞ —Å–ª–µ–¥–Ω–∏—Ç–µ —á–ª–µ–Ω–æ–≤–µ:
{full_text}

–û–±–æ–±—â–∏ –≥–∏ –Ω–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∏, –∫–∞—Ç–æ –≥–æ–≤–æ—Ä–∏—à –≤ —Ç—Ä–µ—Ç–æ –ª–∏—Üe. –§–æ—Ä–º–∞—Ç—ä—Ç –µ markdown.
"""
    output = ask_gemini(prompt)
    return output.strip() if output else "–ù–µ—É—Å–ø–µ—à–Ω–æ –æ–±–æ–±—â–µ–Ω–∏–µ."


def generate_term_with_retries(question):
    for _ in range(MAX_RETRIES):
        term, collections = extract_term_and_collection(question)
        if not term or not collections:
            continue
        matched_indices = find_matching_indices(term, collections)
        if matched_indices:
            return term, matched_indices, []
    return None, [], []


def handle_question(question):
    term, matched_indices, _ = generate_term_with_retries(question)

    if not term:
        return {"error": "–ù–µ –º–æ–∂–µ –¥–∞ —Å–µ –Ω–∞–º–µ—Ä–∏ —Ç–µ—Ä–º–∏–Ω —Å —Ä–µ–∑—É–ª—Ç–∞—Ç–∏."}
    if not matched_indices:
        return {"error": "–ù—è–º–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ –∏–Ω–¥–µ–∫—Å–∏ —Å —Ä–µ–∑—É–ª—Ç–∞—Ç–∏."}

    print(f"üîç Searching for term '{term}' in indices: {matched_indices}")

    detailed_dsl = generate_detailed_dsl(question, term, matched_indices)

    try:
        res = es.search(index=matched_indices, body=detailed_dsl)
    except Exception as e:
        return {"error": f"–ù–µ—É—Å–ø–µ—à–Ω–æ —Ç—ä—Ä—Å–µ–Ω–µ –≤ Elasticsearch: {str(e)}"}

    hits = res.get("hits", {}).get("hits", [])
    if not hits:
        return {"message": f"–ù—è–º–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏ —Ä–µ–∑—É–ª—Ç–∞—Ç–∏ –∑–∞ '{term}'."}

    all_hits, sources = [], []
    for hit in hits:
        desc = hit["_source"].get("description", "")
        chlen_matches = extract_article_context(desc, term)
        all_hits.extend(chlen_matches)
        sources.append({
            "index": hit["_index"],
            "title": hit["_source"].get("title", "–ë–µ–∑ –∑–∞–≥–ª–∞–≤–∏–µ")
        })

    summary = summarize_results(question, all_hits)
    return {
        "term": term,
        "indices": matched_indices,
        "results_count": len(all_hits),
        "summary": summary,
        "sources": sources,
        "matches": all_hits
    }


# üß© FastAPI Routes
class Question(BaseModel):
    question: str


@app.get("/")
def home():
    return {"message": "JusticIA API is running. POST your question to /generate"}


@app.post("/index")
def index_all_data():
    index_mongo_to_es()
    return {"message": "Data indexed successfully."}


@app.post("/generate")
def generate(payload: Question):
    return handle_question(payload.question)
